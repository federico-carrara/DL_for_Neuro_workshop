{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import string\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from utils import  TrainDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset contains the EEG recordings of 15 subjects. For each subject, we have 15 different recordings, each one collected while watching a different movie clip. Each clip is associated to an emotional state amon {sad: -1, neutral: 0, happy: 1}. EEG recordings comprises 62 channels.\n",
    "\n",
    "N.B. Recordings correspondent to the same movies have the same length, while recordings correspondent to different movies have different length (iun general). How to do? No problem, since we are taking sub windows of the signals.\n",
    "\n",
    "Data have been preprocessed by downsampling signals to 200Hz, segmentating the signals such that it corresponds to the length of the movie and applying a band-pass filter at 0-75Hz. Since recordings are about 4 minutes long and are now sampled at 200Hz, they contain roughly 48k time points each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"data/Preprocessed_EEG/1_20131027.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'djc_eeg1', 'djc_eeg2', 'djc_eeg3', 'djc_eeg4', 'djc_eeg5', 'djc_eeg6', 'djc_eeg7', 'djc_eeg8', 'djc_eeg9', 'djc_eeg10', 'djc_eeg11', 'djc_eeg12', 'djc_eeg13', 'djc_eeg14', 'djc_eeg15'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = loadmat(\"data/Preprocessed_EEG/label.mat\")\n",
    "labels[\"label\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.keys())\n",
    "data[\"djc_eeg3\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_files = [fname for fname in os.listdir(\"data/Preprocessed_EEG\") if fname[0] in string.digits]\n",
    "data_lengths = []\n",
    "for eeg_file in tqdm(eeg_files):\n",
    "    raw = loadmat(os.path.join(\"data/Preprocessed_EEG\", eeg_file))\n",
    "    curr_lengths = []\n",
    "    pattern = list(raw.keys())[4].split(\"_\")[0]\n",
    "    for i in range(15):\n",
    "        curr_lengths.append(raw[f\"{pattern}_eeg{i + 1}\"].shape[1])\n",
    "    data_lengths.append(curr_lengths)\n",
    "\n",
    "data_lengths = np.asarray(data_lengths)\n",
    "print(data_lengths.shape)\n",
    "print(np.mean(data_lengths))\n",
    "print(np.mean(data_lengths, axis=0))\n",
    "print(np.mean(data_lengths, axis=1))\n",
    "print(np.unique(data_lengths, return_counts=True))\n",
    "print(np.std(data_lengths, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data files: 100%|██████████| 45/45 [01:06<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = TrainDataset(\"data/Preprocessed_EEG\", \"data/Preprocessed_EEG/label.mat\", 2000, 200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1929, -0.1189,  0.0165],\n",
      "          [ 0.1126,  0.2291, -0.0490],\n",
      "          [ 0.3041, -0.2820, -0.0595]]],\n",
      "\n",
      "\n",
      "        [[[-0.3324,  0.0276,  0.0946],\n",
      "          [-0.1349,  0.1384, -0.0540],\n",
      "          [-0.2897,  0.2559,  0.2055]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1685,  0.2659,  0.1226],\n",
      "          [ 0.1772,  0.2766, -0.0672],\n",
      "          [-0.2600, -0.1639, -0.0445]]],\n",
      "\n",
      "\n",
      "        [[[-0.0366, -0.0023,  0.1910],\n",
      "          [ 0.1069, -0.2465, -0.1001],\n",
      "          [-0.0784,  0.2029, -0.1209]]]], requires_grad=True)\n",
      "Input tensor shape: torch.Size([1, 2, 4, 4])\n",
      "Output tensor shape: torch.Size([1, 4, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.1117, -0.4966,  0.1631, -0.8817],\n",
       "           [ 0.0539,  0.6684, -0.0597, -0.4675],\n",
       "           [-0.2153,  0.8840, -0.7584, -0.3689],\n",
       "           [-0.3424, -1.4020,  0.3206, -1.0219]],\n",
       " \n",
       "          [[ 0.7988, -0.0923, -0.7049, -1.6024],\n",
       "           [ 0.2891,  0.4899, -0.3853, -0.7120],\n",
       "           [-0.1706, -1.4594,  0.2207,  0.2463],\n",
       "           [-1.3248,  0.6970, -0.6631,  1.2158]]]]),\n",
       " tensor([[[[-0.2068,  0.3199],\n",
       "           [ 0.2781, -0.5233]],\n",
       " \n",
       "          [[ 0.2064, -0.5666],\n",
       "           [-0.0600, -0.2478]],\n",
       " \n",
       "          [[ 0.4253, -0.1238],\n",
       "           [-0.1421, -0.5327]],\n",
       " \n",
       "          [[-0.6064, -0.0351],\n",
       "           [ 0.4775, -0.8064]]]], grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple input tensor with 1 channel and size 4x4\n",
    "torch.random.manual_seed(1234)\n",
    "input_tensor = torch.randn(1, 2, 4, 4)  # (batch_size, channels, height, width)\n",
    "\n",
    "# Create a Conv2d layer with 2 groups\n",
    "conv_layer = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=3, groups=2)\n",
    "print(conv_layer.weight)\n",
    "\n",
    "# Perform the convolution\n",
    "output_tensor = conv_layer(input_tensor)\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(\"Input tensor shape:\", input_tensor.shape)\n",
    "print(\"Output tensor shape:\", output_tensor.shape)\n",
    "input_tensor, output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3955072"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 * 12 * 40 * 128 + 128 * 3 + 16 * 64 + 16 * 8 * 32 + 32 * 32 * 16 + 32 * 32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for EEG data analysis\n",
    "We have already seen from a more theoretical point of view what Deep Learning models are and what they are meant for. <br>\n",
    "Now it is time to understand how to use these models with Python!! <br>\n",
    "\n",
    "In the following, we will see a complete pipeline for the task of Emotion Recognition on EEG data using the SEED dataset (Zheng et al., 2015, https://bcmi.sjtu.edu.cn/home/seed). <br>\n",
    "The workflow is composed of the following steps:\n",
    "\n",
    "<span style=\"font-size:24px\">1. Data Loading & Preprocessing</span><br>\n",
    "<span style=\"font-size:24px\">2. Model Definition</span><br>\n",
    "<span style=\"font-size:24px\">3. Model Training</span><br>\n",
    "<span style=\"font-size:24px\">4. Model Evaluation</span><br>\n",
    "\n",
    "We will go over every step, one at the time, discussing the problem and trying to find a solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll report here a brief description of the SEED dataset collection (from https://bcmi.sjtu.edu.cn/home/seed/seed.html):\n",
    "\n",
    "*\"Fifteen Chinese film clips (positive, neutral and negative emotions) were chosen from the pool of materials as stimuli used in the experiments. [...] The duration of each film clip is approximately 4 minutes. Each film clip is well edited to create coherent emotion eliciting and maximize emotional meanings. [...] There is a total of 15 trials (ed. movies) for each experiment. There was a 5 s hint before each clip, 45 s for self-assessment and 15 s to rest after each clip in one session. The order of presentation is arranged in such a way that two film clips that target the same emotion are not shown consecutively. For feedback, the participants were told to report their emotional reactions to each film clip by completing the questionnaire immediately after watching each clip. [...] [EEG signals] were collected with the 62-channel ESI NeuroScan System.\"*\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./images/Data_collection_setup.png\" alt=\"exp_setup\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataset can be summarized as follows:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./images/dataset_schema.png\" alt=\"exp_setup\" width=\"1000\" height=\"500\"></div>\n",
    "\n",
    "Finally, it is worth mentioning that the provided data were **downsampled** at **200Hz** and a **bandpass frequency filter** from **0 - 75 Hz** was applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-size:24px\"><em>A bit of Math</em></span>\n",
    "\n",
    "As we discussed before, Deep Learning is all about Linear Algebra and tensors. Let's write down a *Legend* to fix symbols for the dimensions of our dataset. <br>\n",
    "<div style=\"text-align:center\">\n",
    "    <p style=\"font-size:24px;\">N: <small><em>number of EEG recordings</em></small>, C: <small><em>number of EEG channels</em></small>, W: <small><em>EEG signal length</em></small> B: <small><em>Batch size</em></small></p>\n",
    "</div>\n",
    "\n",
    "Now, if we call the dataset containing the EEG signals as $X$ and the associated labels as $y$, we have tha $X$ and $y$ are tensors of size:\n",
    "    $$X \\in [N \\times C \\times W],$$    $$y \\in [N]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load, Extract and Transform\n",
    "The first step is to **load** data from file, **extract** the meaningful information, and **transform** them into a proper format. \n",
    "\n",
    "<span style=\"color:orange;font-size:18px\">This process require some proficiency with file and data handling, plus some experience with Pytorch tensors manipulation, and, hence, it goes  bit out of the scope of this tutorial. <br>\n",
    "Anyway, for who's interested, the code used to manipulate the input data is available at: https://github.com/federico-carrara/DL_for_Neuro_workshop/blob/main/dataset.py</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data preprocessing\n",
    "<span style=\"color:orange;font-size:20px\"><em>Deep Learning models are powerful feature extractors that can deal with unstructured data. <br></em></span>\n",
    "\n",
    "Well, in principle, that's true. However, in practice, the signal-to-noise ratio for most of data source is very low. <br>\n",
    "So, if we input our model with noisy data, most of the times we end up in a *garbage-in-garbage-out* situation:\n",
    "<div style=\"text-align:center\"><img src=\"./images/gigo.png\" alt=\"gigo\"></div>\n",
    "\n",
    "Therefore, **data preprocessing** is an essential step in all data analysis pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
